{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ナイーブベイズ法 Naive Bayes\n",
    "\n",
    "\n",
    "### Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", *\n",
      "解析, サ変接続\n",
      "し, 自立\n",
      "たい, *\n",
      "テキスト, 一般\n",
      "です, *\n",
      ", *\n"
     ]
    }
   ],
   "source": [
    "mecab = MeCab.Tagger()\n",
    "\n",
    "text = '解析したいテキストです'\n",
    "mecab.parse('')\n",
    "node = mecab.parseToNode(text)\n",
    "while node:\n",
    "    word = node.surface\n",
    "    pos = node.feature.split(',')[1]\n",
    "    print('{0}, {1}'.format(word, pos))\n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing Datasets\n",
    "分かち書きファイルの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def get_data_from_path(filepath):\n",
    "    with open(filepath) as f:\n",
    "        return f.read()\n",
    "\n",
    "def write_data(filepath, data):\n",
    "    f = open(filepath, 'w')\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "\n",
    "def get_wakati(text):\n",
    "    tagger = MeCab.Tagger (\"-Ochasen\")\n",
    "    txt = tagger.parse(text)\n",
    "    lines = txt.split('\\n')\n",
    "    rst = ''\n",
    "    for line in lines:\n",
    "        tmp = line.split('\\t')\n",
    "        if len(tmp) <= 4: continue\n",
    "        h = tmp[3]\n",
    "        if h.startswith('名詞') or h.startswith('動詞') or h.startswith('形容詞'):\n",
    "            rst += tmp[2] + ' '\n",
    "    return rst\n",
    "\n",
    "categories = glob.glob('./text/*')\n",
    "categories_wakati_path = './wakati/'\n",
    "for ctg in categories:\n",
    "    ctg_name = ctg.split('/')[-1]\n",
    "    \n",
    "    texts = glob.glob(ctg + '/*')\n",
    "    if len(texts) < 100: continue\n",
    "    \n",
    "    texts = texts[:100]\n",
    "    \n",
    "    ctg_text = ''\n",
    "    for txt in texts:\n",
    "        data = get_data_from_path(txt)\n",
    "        ctg_text += get_wakati(data) + \"\\n\\n\"\n",
    "    write_data(categories_wakati_path + ctg_name + '.txt', ctg_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words ベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909, 30436)\n",
      "909\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "y = []\n",
    "\n",
    "categories = glob.glob('./wakati/*')\n",
    "for ctg in categories:\n",
    "    ctg_name = ctg.split('/')[-1][:-4]\n",
    "    ctg_data = get_data_from_path(ctg)\n",
    "    articles = ctg_data.split(\"\\n\\n\")\n",
    "    corpus.extend(articles)\n",
    "    y.extend([ctg_name for i in range(len(articles))])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X = X.toarray()\n",
    "print(X.shape)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングデータとテストデータに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X,y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training Model\n",
    "GaussianNB() => 0.68\n",
    "MultinomialNB() => 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior='True')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = naive_bayes.MultinomialNB(alpha=0.1, fit_prior='True' )\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8241758241758241"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.naive_bayes.MultinomialNB\n",
    "#### parameters\n",
    "- alpha: ラプラススムージングの係数\n",
    "- fit_prior: 以前学習した結果を維持するか\n",
    "- class_prior: クラスの事前確率\n",
    "\n",
    "#### attributes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
